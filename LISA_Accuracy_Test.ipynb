{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import bleach\n",
    "import cv2\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, CLIPImageProcessor\n",
    "\n",
    "from model.LISA import LISAForCausalLM\n",
    "from model.llava import conversation as conversation_lib\n",
    "from model.llava.mm_utils import tokenizer_image_token\n",
    "from model.segment_anything.utils.transforms import ResizeLongestSide\n",
    "from utils.utils import (DEFAULT_IM_END_TOKEN, DEFAULT_IM_START_TOKEN,\n",
    "                         DEFAULT_IMAGE_TOKEN, IMAGE_TOKEN_INDEX)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import gradio as gr\n",
    "from utils.data_processing import get_mask_from_json\n",
    "from model.llava.mm_utils import tokenizer_image_token\n",
    "from utils.utils import intersectionAndUnionGPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Arg Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(args):\n",
    "    parser = argparse.ArgumentParser(description=\"LISA chat\")\n",
    "    parser.add_argument(\"--version\", default=\"xinlai/LISA-13B-llama2-v1\")\n",
    "    parser.add_argument(\"--vis_save_path\", default=\"./vis_output\", type=str)\n",
    "    parser.add_argument(\n",
    "        \"--precision\",\n",
    "        default=\"fp16\",\n",
    "        type=str,\n",
    "        choices=[\"fp32\", \"bf16\", \"fp16\"],\n",
    "        help=\"precision for inference\",\n",
    "    )\n",
    "    parser.add_argument(\"--image_size\", default=1024, type=int, help=\"image size\")\n",
    "    parser.add_argument(\"--model_max_length\", default=512, type=int)\n",
    "    parser.add_argument(\"--lora_r\", default=8, type=int)\n",
    "    parser.add_argument(\n",
    "        \"--vision-tower\", default=\"openai/clip-vit-large-patch14\", type=str\n",
    "    )\n",
    "    parser.add_argument(\"--local-rank\", default=0, type=int, help=\"node rank\")\n",
    "    parser.add_argument(\"--load_in_8bit\", action=\"store_true\", default=False)\n",
    "    parser.add_argument(\"--load_in_4bit\", action=\"store_true\", default=False)\n",
    "    parser.add_argument(\"--use_mm_start_end\", action=\"store_true\", default=True)\n",
    "    parser.add_argument(\n",
    "        \"--conv_type\",\n",
    "        default=\"llava_v1\",\n",
    "        type=str,\n",
    "        choices=[\"llava_v1\", \"llava_llama_2\"],\n",
    "    )\n",
    "    return parser.parse_args(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    x,\n",
    "    pixel_mean=torch.Tensor([123.675, 116.28, 103.53]).view(-1, 1, 1),\n",
    "    pixel_std=torch.Tensor([58.395, 57.12, 57.375]).view(-1, 1, 1),\n",
    "    img_size=1024,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Normalize pixel values and pad to a square input.\"\"\"\n",
    "    # Normalize colors\n",
    "    x = (x - pixel_mean) / pixel_std\n",
    "    # Pad\n",
    "    h, w = x.shape[-2:]\n",
    "    padh = img_size - h\n",
    "    padw = img_size - w\n",
    "    x = F.pad(x, (0, padw, 0, padh))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initiation of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = \"\"\n",
    "os.makedirs(args.vis_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.version,\n",
    "    cache_dir=None,\n",
    "    model_max_length=args.model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "args.seg_token_idx = tokenizer(\"[SEG]\", add_special_tokens=False).input_ids[0]\n",
    "\n",
    "torch_dtype = torch.float32\n",
    "if args.precision == \"bf16\":\n",
    "    torch_dtype = torch.bfloat16\n",
    "elif args.precision == \"fp16\":\n",
    "    torch_dtype = torch.half\n",
    "\n",
    "kwargs = {\"torch_dtype\": torch_dtype}\n",
    "if args.load_in_4bit:\n",
    "    kwargs.update(\n",
    "        {\n",
    "            \"torch_dtype\": torch.half,\n",
    "            \"load_in_4bit\": True,\n",
    "            \"quantization_config\": BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                llm_int8_skip_modules=[\"visual_model\"],\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "elif args.load_in_8bit:\n",
    "    kwargs.update(\n",
    "        {\n",
    "            \"torch_dtype\": torch.half,\n",
    "            \"quantization_config\": BitsAndBytesConfig(\n",
    "                llm_int8_skip_modules=[\"visual_model\"],\n",
    "                load_in_8bit=True,\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "model = LISAForCausalLM.from_pretrained(\n",
    "    args.version, low_cpu_mem_usage=True, vision_tower=args.vision_tower, seg_token_idx=args.seg_token_idx, **kwargs\n",
    ")\n",
    "\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.bos_token_id = tokenizer.bos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model.get_model().initialize_vision_modules(model.get_model().config)\n",
    "vision_tower = model.get_model().get_vision_tower()\n",
    "vision_tower.to(dtype=torch_dtype)\n",
    "\n",
    "if args.precision == \"bf16\":\n",
    "    model = model.bfloat16().cuda()\n",
    "elif (\n",
    "    args.precision == \"fp16\" and (not args.load_in_4bit) and (not args.load_in_8bit)\n",
    "):\n",
    "    vision_tower = model.get_model().get_vision_tower()\n",
    "    model.model.vision_tower = None\n",
    "    import deepspeed\n",
    "\n",
    "    model_engine = deepspeed.init_inference(\n",
    "        model=model,\n",
    "        dtype=torch.half,\n",
    "        replace_with_kernel_inject=True,\n",
    "        replace_method=\"auto\",\n",
    "    )\n",
    "    model = model_engine.module\n",
    "    model.model.vision_tower = vision_tower.half().cuda()\n",
    "elif args.precision == \"fp32\":\n",
    "    model = model.float().cuda()\n",
    "\n",
    "vision_tower = model.get_model().get_vision_tower()\n",
    "vision_tower.to(device=args.local_rank)\n",
    "\n",
    "clip_image_processor = CLIPImageProcessor.from_pretrained(model.config.vision_tower)\n",
    "transform = ResizeLongestSide(args.image_size)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import gradio as gr\n",
    "from utils.data_processing import get_mask_from_json\n",
    "from model.llava.mm_utils import tokenizer_image_token\n",
    "from utils.utils import intersectionAndUnionGPU\n",
    "\n",
    "def inference_with_eval(instruction, image_path, json_path):\n",
    "    \"\"\"Process image with LISA and compare to ground truth mask\"\"\"\n",
    "    # Load the image\n",
    "    image_np = cv2.imread(image_path)\n",
    "    image_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n",
    "    original_size_list = [image_np.shape[:2]]\n",
    "    \n",
    "    # Create conversation prompt\n",
    "    if args.use_mm_start_end:\n",
    "        input_str = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "    else:\n",
    "        input_str = DEFAULT_IMAGE_TOKEN\n",
    "    \n",
    "    if instruction:\n",
    "        input_str += \"\\n\" + instruction\n",
    "    \n",
    "    conv = conversation_lib.conv_templates[args.conv_type].copy()\n",
    "    conv.append_message(conv.roles[0], input_str)\n",
    "    conv.append_message(conv.roles[1], \"\")\n",
    "    prompt = conv.get_prompt()\n",
    "    \n",
    "    # Process image for CLIP\n",
    "    image_clip = clip_image_processor.preprocess(image_np, return_tensors=\"pt\")[\"pixel_values\"][0].unsqueeze(0).cuda()\n",
    "    if args.precision == \"bf16\":\n",
    "        image_clip = image_clip.bfloat16()\n",
    "    elif args.precision == \"fp16\":\n",
    "        image_clip = image_clip.half()\n",
    "    else:\n",
    "        image_clip = image_clip.float()\n",
    "    \n",
    "    # Process image for SAM\n",
    "    image = transform.apply_image(image_np)\n",
    "    resize_list = [image.shape[:2]]\n",
    "    image = preprocess(torch.from_numpy(image).permute(2, 0, 1).contiguous()).unsqueeze(0).cuda()\n",
    "    if args.precision == \"bf16\":\n",
    "        image = image.bfloat16()\n",
    "    elif args.precision == \"fp16\":\n",
    "        image = image.half()\n",
    "    else:\n",
    "        image = image.float()\n",
    "    \n",
    "    # Run model inference\n",
    "    input_ids = tokenizer_image_token(prompt, tokenizer, return_tensors=\"pt\")\n",
    "    input_ids = input_ids.unsqueeze(0).cuda()\n",
    "    \n",
    "    output_ids, pred_masks = model.evaluate(\n",
    "        image_clip,\n",
    "        image,\n",
    "        input_ids,\n",
    "        resize_list,\n",
    "        original_size_list,\n",
    "        max_new_tokens=512,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # Get text output\n",
    "    output_ids = output_ids[0][output_ids[0] != IMAGE_TOKEN_INDEX]\n",
    "    text_output = tokenizer.decode(output_ids, skip_special_tokens=False)\n",
    "    text_output = text_output.replace(\"\\n\", \"\").replace(\"  \", \" \")\n",
    "    text_output = text_output.split(\"ASSISTANT: \")[-1]\n",
    "    \n",
    "    # Process prediction mask\n",
    "    pred_output = None\n",
    "    pred_mask_np = None\n",
    "    for i, pred_mask in enumerate(pred_masks):\n",
    "        if pred_mask.shape[0] == 0:\n",
    "            continue\n",
    "            \n",
    "        pred_mask_np = pred_mask.detach().cpu().numpy()[0]\n",
    "        pred_mask_np = pred_mask_np > 0\n",
    "        \n",
    "        # Create prediction visualization\n",
    "        pred_output = image_np.copy()\n",
    "        pred_output[pred_mask_np] = (\n",
    "            image_np * 0.5\n",
    "            + pred_mask_np[:, :, None].astype(np.uint8) * np.array([255, 0, 0]) * 0.5\n",
    "        )[pred_mask_np]\n",
    "        \n",
    "    # Load ground truth mask from JSON\n",
    "    gt_mask, comments, _ = get_mask_from_json(json_path, image_np)\n",
    "    \n",
    "    # Create ground truth visualization  \n",
    "    gt_output = image_np.copy()\n",
    "    gt_output[gt_mask == 1] = (\n",
    "        image_np * 0.5\n",
    "        + (gt_mask == 1)[:, :, None].astype(np.uint8) * np.array([0, 255, 0]) * 0.5\n",
    "    )[gt_mask == 1]\n",
    "    \n",
    "    # Calculate IoU metrics\n",
    "    metrics_text = \"No prediction mask generated\"\n",
    "    if pred_mask_np is not None:\n",
    "        # Convert to tensors\n",
    "        pred_tensor = torch.from_numpy((pred_mask_np).astype(np.int32)).cuda()\n",
    "        gt_tensor = torch.from_numpy(gt_mask).cuda()\n",
    "        \n",
    "        # Calculate intersection and union\n",
    "        intersection, union, acc_iou = intersectionAndUnionGPU(\n",
    "            pred_tensor.contiguous(), \n",
    "            gt_tensor.contiguous(), \n",
    "            2,  # num_classes (0: background, 1: object)\n",
    "            ignore_index=255\n",
    "        )\n",
    "        \n",
    "        # Get CPU values\n",
    "        intersection = intersection.cpu().numpy()\n",
    "        union = union.cpu().numpy()\n",
    "        acc_iou = acc_iou.cpu().numpy()\n",
    "        \n",
    "        # Calculate IoU metrics\n",
    "        ciou = intersection[1] / (union[1] + 1e-10)\n",
    "        giou = acc_iou[1]\n",
    "        \n",
    "        metrics_text = f\"Evaluation Metrics:\\n- Global IoU (gIoU): {giou:.4f}\\n- Class IoU (cIoU): {ciou:.4f}\"\n",
    "    \n",
    "    # If prediction failed, use placeholder\n",
    "    if pred_output is None:\n",
    "        pred_output = cv2.imread(\"/no_seg_out.png\")[:, :, ::-1]\n",
    "        \n",
    "    return gt_output, pred_output, metrics_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Gradio UI setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new Gradio interface\n",
    "demo = gr.Interface(\n",
    "    inference_with_eval,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=1, placeholder=\"Segment the sky\", label=\"Text Instruction\"),\n",
    "        gr.Image(type=\"filepath\", label=\"Input Image\"),\n",
    "        gr.File(label=\"Ground Truth JSON File\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Image(type=\"pil\", label=\"Ground Truth Mask (Green)\"),\n",
    "        gr.Image(type=\"pil\", label=\"Predicted Mask (Red)\"),\n",
    "        gr.Textbox(lines=3, label=\"Evaluation Metrics\")\n",
    "    ],\n",
    "    title=\"LISA Segmentation Accuracy Test\",\n",
    "    description=\"Compare LISA segmentation predictions with ground truth masks\",\n",
    "    examples=[\n",
    "        # Add examples with paths to your test images and corresponding JSON files\n",
    "        [\"Segment the person\", \"test_data/person1.jpg\", \"test_data/person1.json\"],\n",
    "        [\"Segment the cat\", \"test_data/cat.jpg\", \"test_data/cat.json\"]\n",
    "    ],\n",
    "    allow_flagging=\"auto\",\n",
    ")\n",
    "\n",
    "demo.queue()\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
